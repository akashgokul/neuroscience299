{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load modules and libraries\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "RNG = np.random.default_rng(42) #for reproducibility ADDED BY ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b477c22",
   "metadata": {},
   "source": [
    "# Function to generate \"index\" vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebcf7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_word_vector(dimension, k):\n",
    "    #k is a number of +1s and -1s if you decide to implement Random Indexing algorithm. So the total number of nonzero components in the vector is 2k\n",
    "    #For BEAGLE k is not relevant\n",
    "    v = np.array(np.zeros(dimension),np.int8) #Vector in initialized. \n",
    "    \n",
    "    #ToDo Generate components of an \"index\" vector randomly using the randomness suitable for the chosen algorithm\n",
    "    # Per An Introduction to Random Indexing (2005):  These index vectors are sparse [few +1 & -1s, rest 0], high-dimensional\n",
    "    indices_of_ones = RNG.choice(dimension, k, replace=False)\n",
    "    ones_vector = RNG.binomial(1, 0.5, k)\n",
    "    v[indices_of_ones] = 2*ones_vector - 1\n",
    "    \n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aed270",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da17552",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_NAME = \"TOEFL_synonyms.txt\" # file with TOEFL dataset\n",
    "DATA_FILE_NAME = \"Glemmatized.txt\" # file with the text corpus\n",
    "\n",
    "\n",
    "dimension = 500 # Dimensionality for high-dimensional vectors\n",
    "\n",
    "THRESHOLD = 100000 # Frequency threshold in the corpus \n",
    "\n",
    "ONES_NUMBER = 5 # number of nonzero elements in randomly generated high-dimensional vectors\n",
    "window_size = 3 #number of neighboring words to consider both back and forth. In other words number of words before/after current word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d1144",
   "metadata": {},
   "source": [
    "# Initialize \"index\" vectors and embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be004d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making into function so I can run experiments automatically\n",
    "def init_index_vectors(dimension: int = dimension):\n",
    "    # Returns 3 dictionaries\n",
    "    dictionary = {} # vocabulary and corresponing random high-dimensional vectors\n",
    "    amount_dictionary = {} # counts frequency of words\n",
    "    word_space = {} # stores embedings\n",
    "\n",
    "    # Count how many times each word appears in the corpus\n",
    "    text_file = open(DATA_FILE_NAME, \"r\")\n",
    "    for line in text_file:\n",
    "        if line != \"\\n\":\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if amount_dictionary.get(word) is None:\n",
    "                    amount_dictionary[word] = 1\n",
    "                else:\n",
    "                    amount_dictionary[word] += 1\n",
    "    text_file.close()\n",
    "\n",
    "    #Create a dictionary with the assigned random high-dimensional vectors\n",
    "    text_file = open(DATA_FILE_NAME, \"r\")\n",
    "    for line in text_file: #read line in the file\n",
    "        words = line.split() # extract words from the line\n",
    "        for word in words:  # for each word\n",
    "            if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
    "                if amount_dictionary[word] < THRESHOLD:\n",
    "                    dictionary[word] = get_random_word_vector(dimension, ONES_NUMBER) # assign an \"index\" vector \n",
    "                else:\n",
    "                    dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
    "    text_file.close()\n",
    "    return dictionary, amount_dictionary, word_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c555c",
   "metadata": {},
   "source": [
    "# Choose embeddigns to construct  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440e209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_embeddings(word_space: dict, dimension: int = dimension):\n",
    "    #Returns word_space. number_of_tests\n",
    "    \n",
    "    #Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
    "    number_of_tests = 0\n",
    "    TOEFL_file = open(TEST_NAME, \"r\") # open TOEFL file\n",
    "\n",
    "    #Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros    \n",
    "    for line in TOEFL_file:\n",
    "            words = line.split()\n",
    "            word_space[words[0]] = np.zeros(dimension)\n",
    "            word_space[words[1]] = np.zeros(dimension)\n",
    "            word_space[words[2]] = np.zeros(dimension)\n",
    "            word_space[words[3]] = np.zeros(dimension)\n",
    "            word_space[words[4]] = np.zeros(dimension)\n",
    "            number_of_tests += 1 # counts the number of test cases in TOEFL file\n",
    "    TOEFL_file.close()\n",
    "    return word_space, number_of_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1428e54",
   "metadata": {},
   "source": [
    "# Construct embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343d4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embeddings(dictionary: dict, \n",
    "                         word_space: dict,\n",
    "                        dimension: int = dimension, \n",
    "                        window_size: int = window_size,\n",
    "                        technique: str = \"both\") -> dict:\n",
    "    # Technique = \"context\", \"order\", \"both\"\n",
    "    # Returns word_space\n",
    "    \n",
    "    #Each line in the corpus is a sentence so we only consider the window of words within the sentence.\n",
    "    text_file = open(DATA_FILE_NAME, \"r\")\n",
    "\n",
    "    line = \"dummy\" # To avoid skipping while\n",
    "    while line != \"\":\n",
    "        line = text_file.readline()\n",
    "        words = line.split()\n",
    "        for i in range(0,len(words)):\n",
    "            if not (word_space.get(words[i]) is None): # This line forces us to create only embeddigns for words present in TOEFL\n",
    "                #Form \"context\" vector\n",
    "                context=np.zeros(dimension) # initialize context vector\n",
    "                for j in range(max(i-window_size,0),min(i+window_size+1,len(words))): # align window size with the location of the focus word in the sentence\n",
    "                    #ToDo increment context vector with the corresponding \"index\" vectors\n",
    "                    #Note that the index\" vector for the focus word in nor included into the context vector\n",
    "                    if j != i:\n",
    "                        context += dictionary.get(words[j])\n",
    "\n",
    "                #Form \"order\" vector\n",
    "                order=np.zeros(dimension) # initialize order vector\n",
    "                for j in range(max(i-window_size,0), min(i+window_size+1,len(words))): # align window size with the location of the focus word in the sentence\n",
    "                    #ToDo increment context vector with the properly permuted \"index\" vectors\n",
    "                    order += np.roll(dictionary.get(words[j]), j - i)\n",
    "\n",
    "                # Update the embedding with new context and order vectors\n",
    "                if technique == \"context\":\n",
    "                    word_space[words[i]] += context # update the embedding with new context vector        \n",
    "                elif technique == \"order\":\n",
    "                    word_space[words[i]] += order # update the embedding with new order vector \n",
    "                else:\n",
    "                    word_space[words[i]] += context # update the embedding with new context vector   \n",
    "                    word_space[words[i]] += order # update the embedding with new order vector \n",
    "    return word_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b0a6d",
   "metadata": {},
   "source": [
    "# Testing of the embeddings on TOEFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16834e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to check if the answer for TOEFL synonyms task is correct\n",
    "def get_answer_mod(words):\n",
    "    min_value = min(spatial.distance.cosine(words[0], words[1]), spatial.distance.cosine(words[0], words[2]), spatial.distance.cosine(words[0], words[3]),\n",
    "                    spatial.distance.cosine(words[0], words[4]))\n",
    "    if min_value == spatial.distance.cosine(words[0],words[1]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682d18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(amount_dictionary: dict, word_space: dict,\n",
    "             number_of_tests: int, dimension: int = dimension) -> None:\n",
    "    zero_vector = np.zeros(dimension) # used to check if an embedding is non empty \n",
    "    i = 0\n",
    "    TOEFL_file = open(TEST_NAME, 'r')\n",
    "    right_answers = 0.0 # variable for correct answers\n",
    "    number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
    "    while i < number_of_tests:\n",
    "            line = TOEFL_file.readline() #read line in the file\n",
    "            words = line.split()  # extract words from the line\n",
    "            try:\n",
    "                if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
    "                    k = 1\n",
    "                    while k < 5:\n",
    "                        if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
    "                            word_space[words[k]] = np.random.randn(dimension)\n",
    "                        k += 1\n",
    "                    right_answers += get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
    "                                word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
    "            except KeyError: # if there is no representation for the query vector than skip\n",
    "                number_skipped_tests += 1\n",
    "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
    "            except IndexError:\n",
    "                break\n",
    "            i += 1\n",
    "    TOEFL_file.close()\n",
    "    accuracy = 100 * right_answers / number_of_tests # accuracy of the embeddings  \n",
    "    print(\"Dimensionality of embeddings: \" +str(dimension) + \"; Percentage of correct answers in TOEFL: \" + str(accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c26037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def run_experiment(dimension: int = dimension, window_size: int = window_size, technique: str = \"both\") -> None:\n",
    "    print(\"=> Starting Experiment!\")\n",
    "    \n",
    "    start = time.time()\n",
    "    dictionary, amount_dictionary, word_space = init_index_vectors(dimension)\n",
    "    print(f\"=> Computed Dictionaries! Took {(time.time()-start) / 60} min\")\n",
    "    \n",
    "    start = time.time()\n",
    "    word_space, number_of_tests = choose_embeddings(word_space, dimension)\n",
    "    print(f\"=> Setup Embeddings! Took {time.time()-start}\")\n",
    "    \n",
    "        \n",
    "    start = time.time()\n",
    "    word_space = construct_embeddings(dictionary, word_space, dimension, window_size, technique)\n",
    "    print(f\"=> Constructed Embeddings! Took {(time.time()-start) / 60} min\")\n",
    "    \n",
    "    print(f\"=>Testing on D={dimension}, Window = {window_size}, and Technique = {technique}\")\n",
    "    evaluate(amount_dictionary, word_space, number_of_tests, dimension=dimension)\n",
    "    print(\"--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f34c40-c0bc-4e3b-a869-6d24b7853121",
   "metadata": {},
   "source": [
    "## Q1 and Q3\n",
    "### Q1: Get the performance of the embeddings on the TOEFL synonymy assessment for several different dimensionalities.\n",
    "### Q3: Report the accuracy on the TOEFL synonymy assessment for all simulations. Elaborate how accuracy changes with the dimensionality.\n",
    "### From the wording, it seems like Q3 is asking to report results of Q1\n",
    "### ANSWER: As dimensionality increases, performance generally increases but in our case at 1000 the performance drop. In general, greater dimensionality should boost accuracy because greater dimensionality allows us to encode more information and avoid collisions or relative-collisions (in terms of being very similar) between words with different meanings. However, in our case our vectors are very sparse as k = 5 which means that as dimension a greater percent of our vectors are 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d54db3c-3231-4369-aca0-c42179d3dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.952737029393514 min\n",
      "=> Setup Embeddings! Took 0.007855892181396484\n",
      "=> Constructed Embeddings! Took 3.1455177466074624 min\n",
      "=>Testing on D=250, Window = 3, and Technique = both\n",
      "Dimensionality of embeddings: 250; Percentage of correct answers in TOEFL: 40.0%\n",
      "--------------\n",
      "\n",
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.6510003169377645 min\n",
      "=> Setup Embeddings! Took 0.001680135726928711\n",
      "=> Constructed Embeddings! Took 3.136160699526469 min\n",
      "=>Testing on D=500, Window = 3, and Technique = both\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 47.5%\n",
      "--------------\n",
      "\n",
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.6559061686197917 min\n",
      "=> Setup Embeddings! Took 0.0017879009246826172\n",
      "=> Constructed Embeddings! Took 3.506387631098429 min\n",
      "=>Testing on D=1000, Window = 3, and Technique = both\n",
      "Dimensionality of embeddings: 1000; Percentage of correct answers in TOEFL: 42.5%\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Q1\n",
    "for dim in [250, 500, 1000]:\n",
    "    run_experiment(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625d896-ad88-47f2-9208-bb63f00e8d9c",
   "metadata": {},
   "source": [
    "## Q2: (Random Indexing) How does the size of the window around the focus word affect the results on the TOEFL synonymy assessment?\n",
    "### Answer: Increasing window size helps to a point. We see that it is best at size 6 (out of 1,3,6) while there is a drop-off between size 1 and 3. This may be because in a window size like 3 you may be incorporating nearby prepositions or high frequency words of little use (e.g. the, a). Thus there is a \"Goldilocks zone\" where the window size encompasses enough nearby context without that contex encoding too much irrelevant information or too much context in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d31cd88-3f31-459f-b2a6-5e12e8591e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.7441611329714457 min\n",
      "=> Setup Embeddings! Took 0.0018248558044433594\n",
      "=> Constructed Embeddings! Took 1.723079784711202 min\n",
      "=>Testing on D=500, Window = 1, and Technique = both\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 33.75%\n",
      "--------------\n",
      "\n",
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.6857862353324891 min\n",
      "=> Setup Embeddings! Took 0.0015149116516113281\n",
      "=> Constructed Embeddings! Took 3.4392805695533752 min\n",
      "=>Testing on D=500, Window = 3, and Technique = both\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 31.25%\n",
      "--------------\n",
      "\n",
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.7124945521354675 min\n",
      "=> Setup Embeddings! Took 0.0011870861053466797\n",
      "=> Constructed Embeddings! Took 5.669906083742777 min\n",
      "=>Testing on D=500, Window = 6, and Technique = both\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 47.5%\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running Q2\n",
    "for size in [1,3,6]: #Testing window_sizes of 1, 3, 6\n",
    "    run_experiment(window_size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81fd0105-17b4-408b-be60-e97dd4016a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 is Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c9d7b-9d27-461f-b134-4468b6799c19",
   "metadata": {},
   "source": [
    "## Q4: How is accuracy on the TOEFL synonymy assessment affected when only context or only order parts of the embedding are used?\n",
    "### Answer: In this test, we are really concerned with meaning as we are assesssing similarity between synonyms. Thus, order complicates the matter as order is used to enforce grammar-like structure (in terms of relative orders or words) when in this case we just need words and their meanings in context as synonyms will be used interchangably in like contexts. Enforcing order as we do causes a loss of information as by shifting our vectors we are most likely creating new orthogonal vectors to the original index vector of those words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e3d5b6-f33d-48a3-a7ce-1ec267ca45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.8159211317698161 min\n",
      "=> Setup Embeddings! Took 0.007192134857177734\n",
      "=> Constructed Embeddings! Took 3.504171347618103 min\n",
      "=>Testing on D=500, Window = 3, and Technique = context\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 57.5%\n",
      "--------------\n",
      "\n",
      "=> Starting Experiment!\n",
      "=> Computed Dictionaries! Took 0.6906440019607544 min\n",
      "=> Setup Embeddings! Took 0.0009257793426513672\n",
      "=> Constructed Embeddings! Took 3.449483354886373 min\n",
      "=>Testing on D=500, Window = 3, and Technique = order\n",
      "Dimensionality of embeddings: 500; Percentage of correct answers in TOEFL: 38.75%\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "run_experiment(technique=\"context\")\n",
    "run_experiment(technique=\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78f77c-eb71-42ba-aaef-0a8c7b0e8fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6002683-18e2-430e-9d81-5e9401d1c637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TheProcess",
   "language": "python",
   "name": "theprocess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
